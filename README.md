### Overview
This repository contains the Big Data Challenge. The goal is to process and analyze large datasets using Apache Spark and fulfill the requirements provided in the challenge.

### Steps Completed:
1. Downloaded Starter Code:
   - Acquired the starter code from the repository.
2. File Name Changes:
   - Renamed files as per the naming conventions outlined in the challenge requirements.
3. Table Creation:
   - Created the Home_sales table in Apache Spark to process the dataset.
4. Question Solutions:
   - Completed all questions using Spark SQL.
   - Fulfilled all requirements and validated outputs.
 
### Requirements Fulfilled: 
- Properly created the table Home_sales.
- Solved all questions using Spark SQL.
- Verified the output for accuracy and adherence to requirements.
- Cached the home_sales table.
- Created partition by the "date_built" field on the formatted parquet home sales data.
- Created a temporary table for the parquet data.
- Run the last query to determine the runtime and compare it to the uncached runtime.
- Uncached the home_sales temporary table and verified it.
- Downloaded the file to the folder.




